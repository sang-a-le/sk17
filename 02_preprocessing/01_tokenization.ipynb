{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ade45f6",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)\n",
    "- 문장이나 단어를 더 작은 단위로 나누러 분석 가능한 단위(토큰, Token)으로 변환하는 과정\n",
    "- 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 혹은 처리하는 단위로써 토큰 정의 \n",
    "- 자연어 처리에서 크롤링, 데이터 수집 등으로 얻은 코퍼스 데이터는 정제되지 않은 경우가 많은데 이를 사용 용도에 맞게 토큰화, 정제, 정규화하는 과정이 필요\n",
    "\n",
    "**토큰화 목적**\n",
    "- 문법적 구조 이해 \n",
    "- 유연한 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01166ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 기본적인 토큰 처리 지원\n",
    "nltk.download('punkt')   # 토큰 처리 지원 리소스 \n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3fec866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.']\n",
      "['It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['NLP', 'is', 'fascinating', '.'],\n",
       " ['It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text = \"NLP is fascinating. It has many applications in real-world scenarios.\"\n",
    "\n",
    "# 단어 토큰화 \n",
    "word_tokenize(text)\n",
    "\n",
    "# 문장 토큰화 \n",
    "sent_tokenize(text)\n",
    "\n",
    "# 문장별 단어 토큰화 \n",
    "for sentence in sent_tokenize(text):\n",
    "    print(word_tokenize(sentence))\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "tokenize_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594267e",
   "metadata": {},
   "source": [
    "### Subword Tokenization\n",
    "- 하위 단어 토큰화 \n",
    "- BertTokenizer\n",
    "    - 단어를 부분 단위로 쪼개어 희귀하거나 새로운 단어도 부분적으로 표현할 수 있도록 함 → 어휘 크기를 줄이고 다양한 언어 패턴 학습 가능 (어휘: voca의미)\n",
    "    - 인코더에서 발전된 것 (자연어를 매우 잘 이해)- 전 교안 확인(발전 개요 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d90ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e5f23c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un', '##ha', '##pp', '##iness']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# word = 'happy'\n",
    "word = 'unhappiness'\n",
    "subwords = tokenizer.tokenize(word)\n",
    "subwords\n",
    "\n",
    "## tokenizer 설치시 나오는 것들 설명\n",
    "# tokenizer_config : 설정값 가지고 있음. \n",
    "# vocab :  bert가 가지고 있는 vocab 가지고 옴. \n",
    "# tokenizer.json :  단어와 토큰을 결합하기 위한 파일.\n",
    "# config : 해당 토크나이저가 어떤 규칙과 설정을 기반으로 동작하는지 기록한 메타데이터. 모델과 토크나이저가 학습/추론 시에 일관되게 텍스트를 처리할 수 있도록 필요한 정보 가짐. \n",
    "# ['happy'] 반환 = happy를 토큰으로 가지고 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abfa98f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nl',\n",
       " '##p',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'in',\n",
       " 'real',\n",
       " '-',\n",
       " 'world',\n",
       " 'scenarios',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)   # vocb에 존재하는 것은 그대로 반환. 없는 것은 존재하는 최소 단위로 반환 ( != 단어 개념)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f2a06",
   "metadata": {},
   "source": [
    "### 문자 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a57255c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word) # 하나하나의 문자를 배열형태로 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dee8cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'as',\n",
       " 'arroe',\n",
       " 'fruit',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'a',\n",
       " 'banana']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # 정규 표현식 패키지 - 문자열 검색하는 패키지 (내장 패키지)\n",
    "\n",
    "text = 'Time flies like as arroe; fruit flies like a banana.'\n",
    "re.findall(r'\\b\\w+\\b', text)  # findall: text 안에서 전부 찾아내라 / r'\\b\\w+\\b' : 이스케이핑 문자를 이용해서 \\b(경계문자- 공백, 구두점) 문장 형태 표현. \\w(문자, 숫자, 언더바). +(한개 이상) : 앞뒤로 공백이나 구두점이 있는 한글자 이상의 문자를 반환해라 \n",
    "                              # 토큰화를 사용하지 않고도 토큰화한것처럼 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22d5afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'ro', 'use', 'well', '-', 'being', 'practices', 'for', 'self', '-', 'care', '.']\n",
      "['Do', \"n't\", 'hesitate', 'ro', 'use', 'well-being', 'practices', 'for', 'self-care', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer: 단어/구두점으로 토큰을 구분 (',- 포함 단어도 분리)\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text = \"Don't hesitate ro use well-being practices for self-care.\"\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print(word_punct_tokenizer.tokenize(text))  # 문장 부호도 다 분리\n",
    "\n",
    "print(word_tokenize(text))                  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dedf29ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-10', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/08/18', '날짜', '표현에', '사용될', '수', '있다.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n",
      "['COVID-10', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다', '.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/08/18', '날짜', '표현에', '사용될', '수', '있다', '.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, word_tokenize # TreebankWordTokenizer: 표준적인 토큰화 규칙을 따르도록 만들어져있는 패키지\n",
    "\n",
    "text = '''\n",
    "COVID-10(전염병), Dr.Smith(의사), NASA(우주항공국) 등 특정 기관이나 명칭이 있다. \n",
    "특수 문자 또한 태그 <br>, 가격 $100.50, 2025/08/18 날짜 표현에 사용될 수 있다. \n",
    "이러한 경우, $100.50을 하나의 토큰으로 유지할 필요가 있다. \n",
    "'''\n",
    "\n",
    "treebank_word_tokenizer = TreebankWordTokenizer()   \n",
    "print(treebank_word_tokenizer.tokenize(text))   # 널리사용되는 법칙. 일반적으로 한국어에 대해서는 마지막 문장 구문에서 구두점만 구분함. \n",
    "\n",
    "print(word_tokenize(text)) # TreebankWordTokenizer를 차용해서 만들어짐 + 문장 분리 추가 (모든 구두점 분리)\n",
    "# 비슷한 방식으로 구분함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ee888",
   "metadata": {},
   "source": [
    "### 한국어 토큰화\n",
    "- 교착어이기 때문에 단순 띄어쓰기로는 구분 어려움\n",
    "- 형태소 구분이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c0863ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kss==5.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "638d5867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['시간적 배경은 1920년대의 겨울로, 공간적 배경은 경성부.',\n",
       " '주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다.',\n",
       " \"아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다'는 김 첨지의 신조 때문으로 나오지만 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다\",\n",
       " '는 이유가 더 크다.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kss (Koran Sentence Splitter)\n",
    "import kss\n",
    "\n",
    "text = \"시간적 배경은 1920년대의 겨울로, 공간적 배경은 경성부. 주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다. 아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다'는 김 첨지의 신조 때문으로 나오지만 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.\"\n",
    "\n",
    "kss.split_sentences(text)   # 한국어를 제대로 이해했다고 보기는 어려움. bit 그래도 있었다~ 등 한국어 문장 단위를 고려했음을 알수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ed652",
   "metadata": {},
   "source": [
    "### 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a1c86",
   "metadata": {},
   "source": [
    "**pos_tag**\n",
    "\n",
    "pos_tag 는 자영어 처리(NLP)에서 단어에 품사를 태깅하는 함수로, 주로 NLTK와 같은 라이브러리에서 사용된다. \n",
    "\n",
    "** 깃 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b274c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')  # 리소스 설치 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84173933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"Time flies like an arrow.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "pos_tags   # 품사 태깅 현황 반환 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb51fac8",
   "metadata": {},
   "source": [
    "### spacy 주요 품사 태깅 - 깃 확인\n",
    "** nltk는 교육연구용 목적으로 나옴 라이브러리 다른 라이브러리 spacy 사용해보기 (업무처리 과정에서 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.8.7-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.13-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.10-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.6-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from spacy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from spacy) (2.32.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from spacy) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached blis-1.3.0-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached wrapt-1.17.3-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa_trie-1.3.0-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Using cached spacy-3.8.7-cp312-cp312-win_amd64.whl (13.9 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.13-cp312-cp312-win_amd64.whl (24 kB)\n",
      "Using cached preshed-3.0.10-cp312-cp312-win_amd64.whl (116 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "Using cached thinc-8.3.6-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Using cached blis-1.3.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Using cached smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached marisa_trie-1.3.0-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached wrapt-1.17.3-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "\n",
      "   ---- -----------------------------------  3/28 [typing-inspection]\n",
      "   -------- -------------------------------  6/28 [shellingham]\n",
      "   --------------- ------------------------ 11/28 [cloudpathlib]\n",
      "   ------------------ --------------------- 13/28 [blis]\n",
      "   --------------------- ------------------ 15/28 [srsly]\n",
      "   --------------------- ------------------ 15/28 [srsly]\n",
      "   --------------------- ------------------ 15/28 [srsly]\n",
      "   ---------------------- ----------------- 16/28 [smart-open]\n",
      "   ------------------------ --------------- 17/28 [pydantic]\n",
      "   ------------------------ --------------- 17/28 [pydantic]\n",
      "   ------------------------ --------------- 17/28 [pydantic]\n",
      "   --------------------------- ------------ 19/28 [markdown-it-py]\n",
      "   --------------------------- ------------ 19/28 [markdown-it-py]\n",
      "   ---------------------------- ----------- 20/28 [language-data]\n",
      "   ---------------------------- ----------- 20/28 [language-data]\n",
      "   ---------------------------- ----------- 20/28 [language-data]\n",
      "   ---------------------------- ----------- 20/28 [language-data]\n",
      "   ---------------------------- ----------- 20/28 [language-data]\n",
      "   ------------------------------ --------- 21/28 [rich]\n",
      "   ------------------------------ --------- 21/28 [rich]\n",
      "   ------------------------------ --------- 21/28 [rich]\n",
      "   -------------------------------- ------- 23/28 [confection]\n",
      "   ----------------------------------- ---- 25/28 [thinc]\n",
      "   ----------------------------------- ---- 25/28 [thinc]\n",
      "   ----------------------------------- ---- 25/28 [thinc]\n",
      "   ----------------------------------- ---- 25/28 [thinc]\n",
      "   ----------------------------------- ---- 25/28 [thinc]\n",
      "   ------------------------------------- -- 26/28 [weasel]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   -------------------------------------- - 27/28 [spacy]\n",
      "   ---------------------------------------- 28/28 [spacy]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.0 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 rich-14.1.0 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f318be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "spacy.cli.download('en_core_web_sm')  # 영어모델 다운로드 (사용 전 1회는 반드시 다운로드)\n",
    "spacy_nlp = spacy.load('en_core_web_sm')  # 로드 (사용할 때 마다 로드를 해야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed769dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : NOUN\n",
      "flies : VERB\n",
      "like : ADP\n",
      "an : DET\n",
      "arrow : NOUN\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_nlp(text)  \n",
    "\n",
    "for token in tokens : \n",
    "    print(token.text, \":\", token.pos_)  # print tokens 를 했을 시에는 문장의 형태로 반환. 반복문의 형태로 반환해야 태깅된 품사 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041a522",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "- 한국어 자연어 처리를 위한 라이브러리 \n",
    "- 형태소 분석, 품사 태깅, 텍스트 전처리 등 기능 지원\n",
    "- 여러 형태소 분석기 중 적합한 분석기 선택 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d2a8d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Using cached konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Using cached jpype1-1.6.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting lxml>=4.1.0 (from konlpy)\n",
      "  Using cached lxml-6.0.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from konlpy) (2.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Using cached jpype1-1.6.0-cp312-cp312-win_amd64.whl (355 kB)\n",
      "Using cached lxml-6.0.0-cp312-cp312-win_amd64.whl (4.0 MB)\n",
      "Installing collected packages: lxml, JPype1, konlpy\n",
      "\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ---------------------------------------- 3/3 [konlpy]\n",
      "\n",
      "Successfully installed JPype1-1.6.0 konlpy-0.6.0 lxml-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d41661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '점심', '은', '뭘', '먹어', '볼까', '.', '맛있는', '게', '뭐', '지', '?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "text = '오늘 점심은 뭘 먹어볼까. 맛있는 게 뭐지?'\n",
    "\n",
    "okt = Okt()   # 한국어 형태소 분석기의 경우는 자바를 기반으로 만들어졌기 때문에 jvm에 맞춰져 있어야함. (okt, mecab/ 나머지 하나는 c++ 이 설치되어 있어야함. ) \n",
    "              ## 환경 변수 세팅\n",
    "              # temurin jdk 검색. 윈도우 Temurin 17.0.16+8 - 07/23/2025 Zip파일 다운로드 \n",
    "              # skn_17 안에 dev 폴더 생성 압축 해제 (경로에 한글이 없고, 특수문자가 없는 곳으로 지정)\n",
    "              # window+r :  실행창 -> 'sysdm.cpl ,3' -> 시스템 속성 -> 환경변수 -> 시스템 변수 '새로만들기' -> 시스템 변수 path 수정 (JAVA_HOME 추가. 위로 올리기) - 블로그 사진 확인\n",
    "\n",
    "\n",
    "morphs = okt.morphs(text)\n",
    "morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b6de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'Noun'),\n",
       " ('점심', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('뭘', 'Noun'),\n",
       " ('먹어', 'Verb'),\n",
       " ('볼까', 'Verb'),\n",
       " ('.', 'Punctuation'),\n",
       " ('맛있는', 'Adjective'),\n",
       " ('게', 'Noun'),\n",
       " ('뭐', 'Noun'),\n",
       " ('지', 'Josa'),\n",
       " ('?', 'Punctuation')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos를 이용한 품사 태깅\n",
    "pos_tags = okt.pos(text)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ade8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '점심', '뭘', '게', '뭐']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "nouns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
