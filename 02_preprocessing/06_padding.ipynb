{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da56fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch install 방법\n",
    "# torch 사이트에서 긁어서 사용 \n",
    "# !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f9ebe",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "- 자연어 처리에서 각 문장(문서)의 길이는 다를 수 있음 \n",
    "- 그러나 언어모델은 고정된 길이의 데이터를 효율적으로 처리함\n",
    "- 따라서 모든 문장의 길이를 동일하게 맞춰주는 작업이 필요함 == 패딩 \n",
    "\n",
    "\n",
    "**패딩 이점**\n",
    "1. 일관된 입력 형식\n",
    "2. 병렬 연산 최적화\n",
    "3. 유연한 데이터 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf2915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'],\n",
    "                          ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
    "                          ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
    "                          ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
    "                          ['barber', 'went', 'huge', 'mountain']]\n",
    "                            # 전처리 완료된 데이터 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf52ba",
   "metadata": {},
   "source": [
    "### 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500ae246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "class TokenizerForPadding:\n",
    "    def __init__(self, num_words = None, oov_token='<OOV>'):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.word_counts = Counter()\n",
    "        \n",
    "    def fit_on_texts(self, texts):\n",
    "        # 빈도 수 세기\n",
    "        for sentence in texts:\n",
    "            self.word_counts.update(word for word in sentence if word)  # 실제 단어에 대해서만 빈도 수 세기 \n",
    "\n",
    "        # 빈도 수 기반 vocabulary 생성 \n",
    "        vocab = [self.oov_token] + [word for word, _ in self.word_counts.most_common(self.num_words - 2 if self.num_words else None)] # most_common : 빈도 수가 높은 것 부터 반환\n",
    "                                                                                                                                      # -2가 된 이유 알아보기 (설명해주셨는데 못알아들음)\n",
    "\n",
    "        self.word_index = {word: i+1 for i, word in enumerate(vocab)}  # word index 를 vocab에 채워넣음 (인덱스가 0부터 시작하기 떄문에 +1)\n",
    "        self.index_word = {i: word for word, i in self.word_index.items()}\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [[self.word_index.get(word, self.word_index[self.oov_token]) for word in sentence] for sentence in texts] # text에서 문장단위 반환, 문장에서 토큰 단위 반환 > 정수 인코딩 된 결과 가 있으면 return 아니면 oov 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722276dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen=None, padding='pre', truncating='pre', value=0): # 패딩은 기본값이 0임. truncating(잘라주는것)-pre와 post 두가지로 설정 가능\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(seq) for seq in sequences) \n",
    "\n",
    "    padded_sequences = []\n",
    "\n",
    "    for seq in sequences :      # 문장하나씩 반환\n",
    "        if len(seq) > maxlen :  # 정해준 길이보다 클경우 슬라이싱\n",
    "            if truncating == 'pre' :  # 앞에 있는 것을 자르는 것\n",
    "                seq = seq[-maxlen:]\n",
    "            else :             # post\n",
    "                seq = seq[:maxlen]     # 뒤에 것이 잘리는 것 \n",
    "        else :                 # 정해준 길이보다 짧은 경우\n",
    "            pad_length = maxlen - len(seq)\n",
    "            if padding == 'pre':\n",
    "                seq = [value] * pad_length + seq   # 앞에 채우기 \n",
    "            else :            # post\n",
    "                seq = seq + [value] * pad_length   # 뒤에 채우기\n",
    "        padded_sequences.append(seq)\n",
    "    return torch.tensor(padded_sequences)    # 토치 텐서 형태로 반환 > 결과적으로는 모델의 입력값으로 쓰이기 때문에 텐서 형태로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0a3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 9, 6],\n",
       " [2, 4, 6],\n",
       " [10, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 7],\n",
       " [2, 5, 7],\n",
       " [2, 5, 3],\n",
       " [8, 8, 4, 3, 11, 2, 12],\n",
       " [2, 13, 4, 14]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerForPadding(num_words=15)\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2fe38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  2,  6],\n",
       "        [ 0,  0,  0,  0,  2,  9,  6],\n",
       "        [ 0,  0,  0,  0,  2,  4,  6],\n",
       "        [ 0,  0,  0,  0,  0, 10,  3],\n",
       "        [ 0,  0,  0,  3,  5,  4,  3],\n",
       "        [ 0,  0,  0,  0,  0,  4,  3],\n",
       "        [ 0,  0,  0,  0,  2,  5,  7],\n",
       "        [ 0,  0,  0,  0,  2,  5,  7],\n",
       "        [ 0,  0,  0,  0,  2,  5,  3],\n",
       "        [ 8,  8,  4,  3, 11,  2, 12],\n",
       "        [ 0,  0,  0,  2, 13,  4, 14]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='pre', truncating='pre', maxlen=None)  # maxlen을 None 으로 주면 최대 길이값을 반환함(7로 사용) / pre 반환 시 앞에 붙임\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39f48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  6,  0],\n",
       "        [ 2,  9,  6],\n",
       "        [ 2,  4,  6],\n",
       "        [10,  3,  0],\n",
       "        [ 3,  5,  4],\n",
       "        [ 4,  3,  0],\n",
       "        [ 2,  5,  7],\n",
       "        [ 2,  5,  7],\n",
       "        [ 2,  5,  3],\n",
       "        [ 8,  8,  4],\n",
       "        [ 2, 13,  4]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=3)  # post 부여 시 3 뒤에 붙이고 뒤에 자름. \n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65d497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  6,  0],\n",
       "        [ 2,  9,  6],\n",
       "        [ 2,  4,  6],\n",
       "        [10,  3,  0],\n",
       "        [ 5,  4,  3],\n",
       "        [ 4,  3,  0],\n",
       "        [ 2,  5,  7],\n",
       "        [ 2,  5,  7],\n",
       "        [ 2,  5,  3],\n",
       "        [11,  2, 12],\n",
       "        [13,  4, 14]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', truncating='pre', maxlen=3)  # post 부여 시 3 뒤에 붙이고, pre는 앞에 자름. \n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaabe54",
   "metadata": {},
   "source": [
    "### keras Tokenizer 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15490a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8434220c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0],\n",
       "       [ 1,  8,  5],\n",
       "       [ 1,  3,  5],\n",
       "       [ 9,  2,  0],\n",
       "       [ 2,  4,  3],\n",
       "       [ 3,  2,  0],\n",
       "       [ 1,  4,  6],\n",
       "       [ 1,  4,  6],\n",
       "       [ 1,  4,  2],\n",
       "       [ 7,  7,  3],\n",
       "       [ 1, 12,  3]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=3, truncating='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a5ab6",
   "metadata": {},
   "source": [
    "### 어린완자 데이터 샘플 패딩 처리 \n",
    "1. 텍스트 전처리 (토큰화/불용어처리/정제/정규화)\n",
    "2. 정수 인코딩 by Tokenizer (tensorflow.keras)\n",
    "3. 패딩처리 by pad_sequences (tensorflow.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430b051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"The Little Prince, written by Antoine de Saint-Exupéry, is a poetic tale about a young prince who travels from his home planet to Earth. The story begins with a pilot stranded in the Sahara Desert after his plane crashes. While trying to fix his plane, he meets a mysterious young boy, the Little Prince.\n",
    "\n",
    "The Little Prince comes from a small asteroid called B-612, where he lives alone with a rose that he loves deeply. He recounts his journey to the pilot, describing his visits to several other planets. Each planet is inhabited by a different character, such as a king, a vain man, a drunkard, a businessman, a geographer, and a fox. Through these encounters, the Prince learns valuable lessons about love, responsibility, and the nature of adult behavior.\n",
    "\n",
    "On Earth, the Little Prince meets various creatures, including a fox, who teaches him about relationships and the importance of taming, which means building ties with others. The fox's famous line, \"You become responsible, forever, for what you have tamed,\" resonates with the Prince's feelings for his rose.\n",
    "\n",
    "Ultimately, the Little Prince realizes that the essence of life is often invisible and can only be seen with the heart. After sharing his wisdom with the pilot, he prepares to return to his asteroid and his beloved rose. The story concludes with the pilot reflecting on the lessons learned from the Little Prince and the enduring impact of their friendship.\n",
    "\n",
    "The narrative is a beautifully simple yet profound exploration of love, loss, and the importance of seeing beyond the surface of things.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "858475d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 문장 토큰화 \n",
    "sentences = sent_tokenize(raw_text)\n",
    "\n",
    "# 영어 불용어 리스트 \n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')  \n",
    "\n",
    "# 단어사전 (key=단어, value=빈도)\n",
    "vocab = {}\n",
    "\n",
    "\n",
    "# 토큰화/정제/정규화 처리 결과 \n",
    "preprocessed_sentences = []\n",
    "\n",
    "# 토큰 만큼 반복\n",
    "for sentence in sentences : \n",
    "    sentence = sentence.lower() # 대소문자 정규화 (소문자 변환)\n",
    "\n",
    "    tokens = word_tokenize(sentence) # 단어 토큰화\n",
    "    tokens = [token for token in tokens if token not in en_stopwords] # 불용어 제거 \n",
    "    tokens = [token for token in tokens if len(token) > 2]  # 단어 길이가 2 이하면 제거 \n",
    "\n",
    "    for token in tokens :\n",
    "        if token not in vocab:\n",
    "            vocab[token] = 1\n",
    "        else :\n",
    "            vocab[token] += 1\n",
    "    \n",
    "    preprocessed_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78076812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2, 1, 1, 1, 1, 1, 7, 2, 1, 1, 8, 9],\n",
       " [10, 1, 4, 1, 1, 1, 11, 1],\n",
       " [1, 1, 11, 12, 1, 7, 1, 3, 2],\n",
       " [3, 2, 1, 1, 13, 1, 1, 1, 1, 5, 1, 1],\n",
       " [1, 1, 4, 1, 1, 1, 1],\n",
       " [8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6],\n",
       " [1, 2, 1, 1, 14, 1, 1, 1, 1, 1],\n",
       " [9, 3, 2, 12, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [6, 1, 1, 1, 1, 1, 1, 1, 2, 1, 5],\n",
       " [1, 3, 2, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 4, 1, 1, 13, 1, 5],\n",
       " [10, 1, 4, 1, 14, 1, 3, 2, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=15, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequences_t = tokenizer.word_index  # corpus의 모든 단어를 대상으로 생성\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef111b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  2,  1,  1,  1,  1,  1,  7,  2,  1,  1,  8,  9,  0,  0,  0],\n",
       "       [10,  1,  4,  1,  1,  1, 11,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1, 11, 12,  1,  7,  1,  3,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  2,  1,  1, 13,  1,  1,  1,  1,  5,  1,  1,  0,  0,  0,  0],\n",
       "       [ 1,  1,  4,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 8,  1,  1,  1,  1,  1,  1,  1,  1,  1,  6,  0,  0,  0,  0,  0],\n",
       "       [ 1,  2,  1,  1, 14,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 9,  3,  2, 12,  1,  1,  1,  6,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 6,  1,  1,  1,  1,  1,  1,  1,  2,  1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  3,  2,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  4,  1,  1, 13,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10,  1,  4,  1, 14,  1,  3,  2,  1,  1,  1,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=None, truncating='post')\n",
    "padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
